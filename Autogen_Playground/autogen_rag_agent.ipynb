{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysXjb6M0gkhj"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_web_info.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39_S0tWcB-oh"
   },
   "source": [
    "# AutoGen Agents with Retrieval Augmented Generation\n",
    "\n",
    "**`AutoGen`** is a versatile framework that facilitates the creation of LLM applications by employing multiple agents capable of interacting with one another to tackle tasks. These AutoGen agents can be tailored to specific needs, engage in conversations, and seamlessly integrate human participation. They are adaptable to different operation modes that encompass the utilization of LLMs, human inputs, and various tools.\n",
    "\n",
    "**`RAG`** stands for `Retrieval Augmented Generation`, a natural language processing (NLP) technique that combines two essential components: **retrieval** and **generation**.\n",
    "\n",
    "The previous tutorial [AutoGen + LangChain = Super AI Agents](https://github.com/sugarforever/LangChain-Advanced/blob/main/Integrations/AutoGen/autogen_langchain_uniswap_ai_agent.ipynb) introduced how to build an AutoGen application that can execute tasks requiring specific documents knowledge. This is a typical RAG use case, aka. document based chatbot.\n",
    "\n",
    "The latest **AutoGen** version already supports RAG natively with the feature package `retrievechat`.\n",
    "\n",
    "In this tutorial, we are going to rebuild the same feature demonstrated in the previous tutorial. We will utilize `AutoGen` `retrievechat` feature.\n",
    "\n",
    "This tutorial is inspired by the [Blog - Retrieval-Augmented Generation (RAG) Applications with AutoGen](https://microsoft.github.io/autogen/blog/2023/10/18/RetrieveChat) of [Li Jiang](https://github.com/thinkall).\n",
    "\n",
    "Credits go to Li Jiang! üôå\n",
    "\n",
    "Let's roll! üö¥üèª‚Äç‚ôÄÔ∏è üö¥üèª üö¥üèª‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtvjgnBZZjUL"
   },
   "source": [
    "## Use Case\n",
    "\n",
    "\n",
    "\n",
    "In this tutorial, I will create the retrieval augmented agents with the following document:\n",
    "\n",
    "[RETRIEVAL AUGMENTED GENERATION AND REPRESENTATIVE\n",
    "VECTOR SUMMARIZATION FOR LARGE UNSTRUCTURED\n",
    "TEXTUAL DATA IN MEDICAL EDUCATION](https://arxiv.org/pdf/2308.00479.pdf)\n",
    "\n",
    "You should be able to see the agents are able to perform retrieval augmented generation based on the document above and answer question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z73o7bmtb5LH"
   },
   "source": [
    "### Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.317406Z",
     "iopub.status.busy": "2023-02-13T23:40:52.316561Z",
     "iopub.status.idle": "2023-02-13T23:40:52.321193Z",
     "shell.execute_reply": "2023-02-13T23:40:52.320628Z"
    },
    "id": "1VRZnGGGgkhl"
   },
   "outputs": [],
   "source": [
    "%pip install pyautogen[retrievechat] langchain \"chromadb<0.4.15\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZ7w_A3nXU8-"
   },
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA48TH6Hc_3c"
   },
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCrCnRC7cdC-"
   },
   "source": [
    "#### 1. Configure Embedding Function\n",
    "\n",
    "We will use OpenAI embedding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c636oZ2zHNpr"
   },
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "openai_embedding_function = embedding_functions.OpenAIEmbeddingFunction(api_key = config_list[0][\"api_key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxFsXiHVciOo"
   },
   "source": [
    "#### 2. Configure Text Splitter\n",
    "\n",
    "LangChain has done a great job in text splitting, so we will use its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eRvVjJITKfR"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\r\", \"\\t\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7VPXVI_coX4"
   },
   "source": [
    "#### 3. Configure Vector Store\n",
    "\n",
    "By default, the AutoGen retrieval augmented agents use `chromadb` as vector store.\n",
    "\n",
    "Developers can configure preferred vector store by extending the class `RetrieveUserProxyAgent` and overriding function `retrieve_docs`.\n",
    "\n",
    "AutoGen also supports simple configuration items to customize Chromadb storage.\n",
    "\n",
    "In this demo, we will specify the collection name by `retreive_config` item `collection_name`. You should be able to see it in step 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu7gjAv-c4uP"
   },
   "source": [
    "#### 4. Create Retrieval Augmented Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZsXuHf1fgkhl"
   },
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "\n",
    "llm_config = {\n",
    "    \"request_timeout\": 600,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "rag_agent = RetrieveUserProxyAgent(\n",
    "    human_input_mode=\"NEVER\",\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": \"./rag.pdf\",\n",
    "        \"collection_name\": \"rag_collection\",\n",
    "        \"embedding_function\": openai_embedding_function,\n",
    "        \"custom_text_split_function\": text_splitter.split_text,\n",
    "    },\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37cRtpqLdLSZ"
   },
   "source": [
    "### It's time to start a chat with the RAG agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCdAqig3gkhn",
    "outputId": "d4463f2e-7dcc-4fa1-8552-b9dab3291afd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autogen.retrieve_utils:Found 3 chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n",
      "doc_ids:  [['doc_1', 'doc_2']]\n",
      "Adding doc_id doc_1 to context.\n",
      "Adding doc_id doc_2 to context.\n",
      "RetrieveChatAgent (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "You must give as short an answer as possible.\n",
      "\n",
      "User's question is: What is the workflow in docGPT?\n",
      "\n",
      "Context is: is initially stored in memory as a Document and then the Document is split into text chunks using recursive character text\n",
      "splitting. The chunks are then embedded in a 1536-dimensional vector space using OpenAI text-embedding-ada-002\n",
      "embedding engine and stored in Facebook AI Similarity Search ( FAISS ) vector database [ 3]. The FAISS vector database\n",
      "is used to find the k-most similar chunks to a given query at the query time. The original query, combined with the\n",
      "retrieved chunks is compiled into a prompt and passed to the LLM for generating the answer. The model workflow and\n",
      "the steps excecuted at build-time and at query-time are depicted in Figure 1.\n",
      "2.2 Summarization\n",
      "2.2.1 Representative Vector\n",
      "A maximum affordable token limit ( T) is initially defined. The target is to select knumber of chunks which is the\n",
      "maximum number of chunks that can be selected without their total token size exceeding T, from the vector database of\n",
      "nchunks. Assuming each chunk has an average token size of s,kis simply obtained by optimizing for max k‚â§n,k√ós‚â§Tk\n",
      "Once kis calculated, the chunks in the high-dimension vector space are quantized using the k-means clustering\n",
      "algorithm. k-number of clusters which minimize the within-cluster sum of squared Euclidian distances from the\n",
      "corresponding centroids are created within the vector space. Since the distribution of chunks in the vector space is based\n",
      "on their contextual similarity, we assume each cluster captures different aspects/semantics of the original document.\n",
      "After this quantization, one representative chunk, which is the closest to the corresponding centroid from each cluster is\n",
      "extracted.\n",
      "Let vectors be an n√ódmatrix, where nis the number of chunks and dis the dimensionality of the embeddings. Let\n",
      "centroids [i]denote the centroid of i-th cluster. The Euclidian distance between the m-th data point and the centroid of\n",
      "thei-th cluster is calculated by\n",
      "distance m=vuutdX\n",
      "j=1(vectors [m][j]‚àícentroids [i][j])2 (1)\n",
      "and stored in the distances [i]array.\n",
      "Next, the index of the data point that has the minimum distance to the centroid of the i-th cluster, denoted as\n",
      "closest _index i, is calculated by:\n",
      "closest _index i= argmin distances i (2)\n",
      "Finally, the closest _index ivalues for all clusters are stored in a list.\n",
      "closest _indices = [closest _index 0, closest _index 1, . . . , closest _index k‚àí1]\n",
      "Once the indices of the representative chunks are identified, they are stored in a separate representative Document list.\n",
      "2.2.2 Keyword generation and mapping\n",
      "Even though the token size of the representative text chunks obtained with this method is relatively smaller compared to\n",
      "the original document, it can still be too large to fit into the LLM‚Äôs context window. For example, docGPT uses a default\n",
      "maximum affordable token limit of 10,000 tokens. The default LLM that docGPT calls is OpenAI gpt-3.5-turbo ,\n",
      "which has a maximum context window of 4k. Therefore, the representative Document list undergoes an intermediate\n",
      "step of extractive summarization (keyword generation and mapping) for each chunk in it.\n",
      "Three keywords are generated for each representative chunk and the keywords are distributed among all the other\n",
      "members of the same cluster. A word cloud is generated from all the keywords from the n chunks. The relative\n",
      "frequency that each keyword appears is reflected by the size of the word in the word cloud.\n",
      "2AI for Medicine\n",
      "Figure 1: Retrieval workflow of docGPT\n",
      "3AI for Medicine\n",
      "Figure 2: Word cloud for Kumar and Clark Clinical Medicine\n",
      "The original high-dimensional vectors are then reduced to two dimensions using t-distributed Stochastic Neighbor\n",
      "Embedding (t-SNE) and plotted on a scatter plot with colors corresponding to their clusters and keywords. This enables\n",
      "the identification of the distribution of the contents of the entire document at a glance.\n",
      "Finally, the mapped summaries are used to create a final abstractive summary and to create a list of key points from the\n",
      "documents.\n",
      "3 Evaluation\n",
      "3.1 Retrieval\n",
      "Finally, the mapped summaries are used to create a final abstractive summary and to create a list of key points from the\n",
      "documents.\n",
      "3 Evaluation\n",
      "3.1 Retrieval\n",
      "Responses for queries on clinical medicine and general pharmacology from LLM without a non-parametric knowledge-\n",
      "base and a RAG model with vector databases built from Kumar and Clark‚Äôs Clinical Medicine 10th Edition and British\n",
      "National Formulary 82 were compared and checked for accuracy [ 6,7]. The responses generated by chatGPT and\n",
      "RAG implementation in docGPT are tabulated with the excerpts from the reference books in the supplementary material\n",
      "named Queries. For each query, docGPT generated more targetted and accurate answers, while chtGPT answers were\n",
      "more generic.\n",
      "3.2 Summarization\n",
      "Performances of RVS and retrieve-stuff-summarize methods were tested using Kumar and Clark Clinical Medicine 10th\n",
      "edition and BNF 82 ebooks. Results are summarized in supplementary material named Summaries.\n",
      "Kumar and Clark had 1508 pages with 13024 text chunks, each having an average of 789 tokens. The maximum\n",
      "affordable token limit was set at 15,000 and 19 representative chunks were selected. The word cloud and the t-SNE\n",
      "visualization are depicted in Figures 2 and 3.\n",
      "BNF had 1805 pages with 7278 text chunks with an average token size of 486. The model chose 10 representative\n",
      "chunks under the constraints of 5000 maximum affordable tokens. The word cloud and t-SNE are depicted in Figures 4\n",
      "and 5.\n",
      "4 Implementation\n",
      "RVS is implemented in docGPT , a document intelligence program written in Python using langchain framework and\n",
      "the source is available at https://github.com/ssm123ssm/docGPT-pharm .\n",
      "5 Conclusion\n",
      "Clinical medicine is a knowledge-intensive domain. Both clinicians and medical students would benefit from efficient\n",
      "methods for retrieving information quickly from large knowledgebases. We believe the proposed retrieval augmented\n",
      "4AI for Medicine\n",
      "Figure 3: t-SNE visualization of clustering of chunks in embedding space for Kumar and Clark Clinical Medicine\n",
      "Figure 4: Word cloud BNF 82\n",
      "Figure 5: t-SNE visualization of clustering of chunks in embedding space for BNF\n",
      "5AI for Medicine\n",
      "generation workflow and representative vector summarization for large documents would be of help in this context.\n",
      "Even though the workflow was tested on medical reference books and use cases related to medical education, the\n",
      "concept of RAG and RVS can be adopted by other domains as well.\n",
      "References\n",
      "[1]Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales,\n",
      "Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly,\n",
      "Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S. Corrado,\n",
      "Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher\n",
      "Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge.\n",
      "[2]Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler,\n",
      "Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation\n",
      "for knowledge-intensive NLP tasks.\n",
      "[3] Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. Billion-scale similarity search with GPUs.\n",
      "[4] Harrison Chase. LangChain. original-date: 2022-10-17T02:58:36Z.\n",
      "[5]R. Smith. An overview of the tesseract OCR engine. In Ninth International Conference on Document Analysis and\n",
      "Recognition (ICDAR 2007) Vol 2 , pages 629‚Äì633. IEEE. ISSN: 1520-5363.\n",
      "[6] Kumar and clark‚Äôs clinical medicine, 10th edition - 9780702078682.\n",
      "[7] British national formulary | BNF publications.\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "assistant (to RetrieveChatAgent):\n",
      "\n",
      "The workflow in docGPT involves storing a document in memory, splitting it into text chunks, and embedding these chunks into a vector space using OpenAI's text-embedding engine. These embedded chunks are stored in a FAISS vector database. At query time, the k-most similar chunks to a given query are retrieved from the database. The original query and retrieved chunks are compiled into a prompt and passed to the Language Model for generating the answer. The workflow also includes a summarization process where representative chunks are selected, keywords are generated, and a final abstractive summary is created.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "assistant.reset()\n",
    "rag_agent.initiate_chat(assistant, problem=\"What is the workflow in docGPT?\", n_results=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autogen Flowise LangChain together RAG bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flowise‰Ωú‰∏∫‰Ωé‰ª£Á†ÅÊó†‰ª£Á†ÅÁöÑAIÊµÅÂàõÂª∫Â∑•ÂÖ∑ÔºåËøÖÈÄüÊûÑÂª∫RAGÁöÑAI ÊµÅÁ®ãÔºåÁÑ∂ÂêéÊö¥Èú≤Âá∫APIÔºåÊèê‰æõÁªôAutogen‰Ωú‰∏∫Tool‰ΩøÁî®\n",
    "\n",
    "FlowiseÁîüÊàêÁöÑÊé•Âè£Â¶Ç‰∏ãÔºö\n",
    "\n",
    "``` python\r\n",
    "import requests\r\n",
    "\r\n",
    "API_URL = \"http://localhost:4000/api/v1/prediction/433ed37e-9546-4e73-a688-7352b78bf852\"\r\n",
    "\r\n",
    "def query(payload):\r\n",
    "    response = requests.post(API_URL, json=payload)\r\n",
    "    return response.json()\r\n",
    "\r\n",
    "output = query({\r\n",
    "    \"question\": \"Hey, how are you?\",\r\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÈÄöËøáËá™ÂÆö‰πâÂáΩÊï∞ÔºåÊèê‰æõÁªôAutogenÊ°ÜÊû∂‰ΩøÁî®\n",
    "\n",
    "``` python\n",
    "import requests\n",
    "\n",
    "API_URL = \"http://localhost:4000/api/v1/prediction/433ed37e-9546-4e73-a688-7352b78bf852\"\n",
    "\n",
    "def answer_flowise_question(question):\n",
    "  response = requests.post(API_URL, json={ \"question\": question })\n",
    "  return response.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ËÆæÁΩÆAutoGen UserProxyAgentÂíåAssistantAgent: \n",
    "\n",
    "``` python\n",
    "llm_config={\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"answer_flowise_question\",\n",
    "            \"description\": \"Answer any related questions\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The question to ask in relation to my document\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"question\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\"work_dir\": \".\"},\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.\n",
    "Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\",\n",
    "    function_map={\"answer_flowise_question\": answer_flowise_question}\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Áé∞Âú®Â∞±ÂèØ‰ª•Áî®user_proxyÊèê‰∫§‰ªªÂä°‰∫Ü„ÄÇ‰ª£Á†ÅÂ¶Ç‰∏ãÔºö\n",
    "\n",
    "``` python\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"\"\"\n",
    "I'm writing a blog to introduce the version 3 of Uniswap protocol. Find the answers to the 3 questions below and write an introduction based on them.\n",
    "\n",
    "1. What is Uniswap?\n",
    "2. What are the main changes in Uniswap version 3?\n",
    "3. How to use Uniswap?\n",
    "\n",
    "Start the work now.\n",
    "\"\"\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
