{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfMnSy9vRXqr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## È´òÁ∫ßRAGÊñπÊ≥ï"
      ],
      "metadata": {
        "id": "kauyF35jSVUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small-to-Big Retrieval\n",
        "1. Parent Document Retrieval\n",
        "2. Auto-Merging Retrieval\n",
        "3. Sentence-Window Retrieval"
      ],
      "metadata": {
        "id": "0fWWVJyGSc7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv llama-index trulens-eval torch sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xyJiAQ8S_4Q",
        "outputId": "51b8e0e9-5176-47c0-b4a1-4f1b02389218"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.9.24)\n",
            "Requirement already satisfied: trulens-eval in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.12.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.6.3)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.2.14)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.26.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.0)\n",
            "Requirement already satisfied: frozendict>=2.3.8 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (2.3.10)\n",
            "Requirement already satisfied: munch>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (4.0.0)\n",
            "Requirement already satisfied: dill>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (2.5.3)\n",
            "Requirement already satisfied: merkle-json>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (1.0.0)\n",
            "Requirement already satisfied: langchain>=0.0.335 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (0.0.353)\n",
            "Requirement already satisfied: millify>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (0.1.1)\n",
            "Requirement already satisfied: humanize>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (4.7.0)\n",
            "Requirement already satisfied: streamlit>=1.27.0 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (1.29.0)\n",
            "Requirement already satisfied: streamlit-aggrid>=0.3.4.post3 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (0.3.4.post3)\n",
            "Requirement already satisfied: streamlit-extras>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (0.3.6)\n",
            "Requirement already satisfied: alembic>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from trulens-eval) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index) (4.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.11.2->trulens-eval) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.335->trulens-eval) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.335->trulens-eval) (0.0.7)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.335->trulens-eval) (0.1.4)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.70 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.335->trulens-eval) (0.0.76)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index) (3.20.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index) (1.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index) (1.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index) (3.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->trulens-eval) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->trulens-eval) (2.14.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit>=1.27.0->trulens-eval) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (5.3.2)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (6.11.0)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (10.0.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (2.8.2)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (13.7.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (0.10.2)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (5.2)\n",
            "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (0.22.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (3.1.40)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (0.8.1b0)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (6.3.2)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit>=1.27.0->trulens-eval) (3.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: python-decouple<4.0,>=3.6 in /usr/local/lib/python3.10/dist-packages (from streamlit-aggrid>=0.3.4.post3->trulens-eval) (3.8)\n",
            "Requirement already satisfied: entrypoints>=0.4 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (0.4)\n",
            "Requirement already satisfied: htbuilder>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (0.6.2)\n",
            "Requirement already satisfied: markdownlit>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (0.0.7)\n",
            "Requirement already satisfied: st-annotated-text>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (4.0.1)\n",
            "Requirement already satisfied: streamlit-camera-input-live>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (0.2.0)\n",
            "Requirement already satisfied: streamlit-card>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (1.0.0)\n",
            "Requirement already satisfied: streamlit-embedcode>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (0.1.2)\n",
            "Requirement already satisfied: streamlit-faker>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (0.0.3)\n",
            "Requirement already satisfied: streamlit-image-coordinates<0.2.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (0.1.6)\n",
            "Requirement already satisfied: streamlit-keyup>=0.1.9 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (0.2.2)\n",
            "Requirement already satisfied: streamlit-toggle-switch>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (1.0.2)\n",
            "Requirement already satisfied: streamlit-vertical-slider>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from streamlit-extras>=0.2.7->trulens-eval) (2.5.5)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=1.27.0->trulens-eval) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit>=1.27.0->trulens-eval) (0.12.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index) (1.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.27.0->trulens-eval) (4.0.11)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from htbuilder>=0.6.2->streamlit-extras>=0.2.7->trulens-eval) (10.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit>=1.27.0->trulens-eval) (3.17.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.335->trulens-eval) (2.4)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.2.7->trulens-eval) (3.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.2.7->trulens-eval) (4.9.4)\n",
            "Requirement already satisfied: favicon in /usr/local/lib/python3.10/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.2.7->trulens-eval) (0.7.0)\n",
            "Requirement already satisfied: pymdown-extensions in /usr/local/lib/python3.10/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.2.7->trulens-eval) (10.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit>=1.27.0->trulens-eval) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=1.27.0->trulens-eval) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit>=1.27.0->trulens-eval) (2.16.1)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (from streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens-eval) (22.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens-eval) (3.7.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.27.0->trulens-eval) (5.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.27.0->trulens-eval) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.27.0->trulens-eval) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.27.0->trulens-eval) (0.15.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=1.27.0->trulens-eval) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens-eval) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens-eval) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens-eval) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens-eval) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->streamlit-faker>=0.0.2->streamlit-extras>=0.2.7->trulens-eval) (3.1.1)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=e5495c868f4b5a0fa0758628140c22713ea26a9872f96e6305533bdfdb898091\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, logging\n",
        "from google.colab import userdata\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "T7wHf1trWtTm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Âü∫Á∫øRAG"
      ],
      "metadata": {
        "id": "ibjdG8EgTAOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index import ServiceContext, StorageContext\n",
        "from llama_index.embeddings import OpenAIEmbedding, HuggingFaceEmbedding\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index import load_index_from_storage\n",
        "\n",
        "from typing import List\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "xKRf_9d1TjTs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create LLM and Embedding Model\n",
        "embed_model = OpenAIEmbedding() # default embedding model ada\n",
        "llm = OpenAI(api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    embed_model=embed_model, llm=llm\n",
        ")\n",
        "\n",
        "# check if data indexes already exists\n",
        "if not os.path.exists(\"./storage\"):\n",
        "    # load data\n",
        "    documents = SimpleDirectoryReader(input_dir=\"dataFiles\").load_data(show_progress=True)\n",
        "\n",
        "    # create nodes parser\n",
        "    node_parser = SimpleNodeParser.from_defaults(chunk_size=1024)\n",
        "\n",
        "    # split into nodes\n",
        "    base_nodes = node_parser.get_nodes_from_documents(documents=documents)\n",
        "\n",
        "    # creating index\n",
        "    index = VectorStoreIndex(nodes=base_nodes, service_context=service_context)\n",
        "\n",
        "    # store index\n",
        "    index.storage_context.persist()\n",
        "else:\n",
        "    # load existing index\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "    index = load_index_from_storage(storage_context=storage_context)\n",
        "\n",
        "\n",
        "# create retriever\n",
        "retriever = index.as_retriever(similarity_top_k=2)\n",
        "\n",
        "# query retriever engine\n",
        "query_engine = RetrieverQueryEngine.from_args(\n",
        "    retriever=retriever,\n",
        "    service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS16lUqBXHsw",
        "outputId": "dc50edf5-101c-40e1-b6bd-390a8e26a925"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 323.86file/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test response\n",
        "response = query_engine.query(\"What did the president say about covid-19\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-i_K2oMXrMt",
        "outputId": "5f154ba8-cc58-4839-97f8-bc255f790a38"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The president stated that COVID-19 need no longer control our lives and that we will never just accept living with COVID-19. He emphasized the importance of continuing to combat the virus and staying on guard, as it is a virus that mutates and spreads. He also mentioned the effectiveness of vaccines and treatments in providing protection against COVID-19 and expressed the commitment to vaccinating more Americans. Additionally, he acknowledged the eagerness of parents with children under 5 to see a vaccine authorized for their children.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Pipeline Evaluation"
      ],
      "metadata": {
        "id": "Tx4Pmp4OXmo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Answer Relevance: How relevant is the answer to the query or the user question?\n",
        "\n",
        "- Context Relevance: How relevant was the retrieved context in regards to answering the user question?\n",
        "\n",
        "- Groundedness: How much is the response supported by the retrieved context?"
      ],
      "metadata": {
        "id": "p4Fhi9owX4y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens_eval import Feedback, Tru, TruLlama, Select\n",
        "from trulens_eval.feedback import Groundedness\n",
        "from trulens_eval.feedback.provider.openai import OpenAI as OpenAITruLens\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "benchmark_result_db = 'benchmark.sqlite'\n",
        "\n",
        "tru = Tru(database_file=benchmark_result_db)\n",
        "\n",
        "fopenai = OpenAITruLens() # default using GPT3.5-turbo for eval\n",
        "\n",
        "grounded = Groundedness(groundedness_provider=OpenAITruLens())\n",
        "# Define a groundedness feedback function\n",
        "f_groundedness = Feedback(grounded.groundedness_measure_with_cot_reasons).on(\n",
        "    TruLlama.select_source_nodes().node.text\n",
        "    ).on_output(\n",
        "    ).aggregate(grounded.grounded_statements_aggregator)\n",
        "\n",
        "# Question/answer relevance between overall question and answer.\n",
        "f_qa_relevance = Feedback(fopenai.relevance).on_input_output()\n",
        "\n",
        "# Question/statement relevance between question and each context chunk.\n",
        "f_context_relevance = Feedback(fopenai.qs_relevance).on_input().on(\n",
        "    TruLlama.select_source_nodes().node.text\n",
        "    ).aggregate(np.mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr2Sq9ufp-6i",
        "outputId": "d4fa0f59-38ec-438f-996f-f43c6a484aba"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:trulens_eval.tru:Tru was already initialized. Cannot change database_url=None or database_file=benchmark.sqlite .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ In groundedness_measure_with_cot_reasons, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
            "‚úÖ In groundedness_measure_with_cot_reasons, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "‚úÖ In relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "‚úÖ In relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "‚úÖ In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
            "‚úÖ In qs_relevance, input statement will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tru_query_engine_recorder = TruLlama(query_engine,\n",
        "    app_id='RAG_Baseline_V0',\n",
        "    feedbacks=[f_groundedness, f_qa_relevance, f_context_relevance])\n",
        "\n",
        "eval_questions = []\n",
        "\n",
        "eval_questions_file = 'eval_questions.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A_N9I1fYILn",
        "outputId": "af4b4bf3-b73c-47af-e7ba-6564770aec43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-b71270106c0a>:9: DeprecationWarning: `database_file` is deprecated, use `database_url` instead as in `database_url='sqlite:///filename'.\n",
            "  tru = Tru(database_file=benchmark_result_db)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶ë Tru initialized with db url sqlite:///benchmark.sqlite .\n",
            "üõë Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n",
            "‚úÖ In groundedness_measure_with_cot_reasons, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
            "‚úÖ In groundedness_measure_with_cot_reasons, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "‚úÖ In relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "‚úÖ In relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "‚úÖ In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
            "‚úÖ In qs_relevance, input statement will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(eval_questions_file, \"r\") as eval_qn:\n",
        "    for qn in eval_qn:\n",
        "        qn_stripped = qn.strip()\n",
        "        eval_questions.append(qn_stripped)\n",
        "\n",
        "\n",
        "def run_eval(eval_questions: List[str]):\n",
        "    for qn in eval_questions:\n",
        "        # eval using context window\n",
        "        with tru_query_engine_recorder as recording:\n",
        "            query_engine.query(qn)\n",
        "\n",
        "\n",
        "run_eval(eval_questions=eval_questions)\n",
        "\n",
        "# run dashboard\n",
        "tru.run_dashboard()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4wghqWEYdkj",
        "outputId": "a4702690-ce3c-4417-8c0f-064505e9a4eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dashboard ...\n",
            "npx: installed 22 in 4.5s\n",
            "\n",
            "Go to this url and submit the ip given here. your url is: https://fresh-badgers-tap.loca.lt\n",
            "\n",
            "  Submit this IP Address: 34.73.66.145\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Comment"
      ],
      "metadata": {
        "id": "D0v4ovKqY2NH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RAG application seems to be performing poorly in retrieving the most relevant document."
      ],
      "metadata": {
        "id": "lmBlE7p1Y565"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto-Merging Retrieval"
      ],
      "metadata": {
        "id": "Up3E7fXcZheW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output a hierarchy of nodes, from top-level nodes with bigger chunk sizes to child nodes with smaller chunk sizes, where each child node has a parent node with a bigger chunk size.\n",
        "\n",
        "By default, the hierarchy is:\n",
        "\n",
        "- 1st level: chunk size 2048\n",
        "- 2nd level: chunk size 512\n",
        "- 3rd level: chunk size 128"
      ],
      "metadata": {
        "id": "U8WHnm4OZrqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from llama_index import (\n",
        "    Document,\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.retrievers import RecursiveRetriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index.embeddings import OpenAIEmbedding\n",
        "from llama_index.schema import IndexNode\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "## Create LLM and Embedding Model\n",
        "embed_model = OpenAIEmbedding() # default embedding model ada\n",
        "llm = OpenAI(api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    embed_model=embed_model, llm=llm\n",
        ")\n",
        "\n",
        "# load data\n",
        "documents = SimpleDirectoryReader(input_dir=\"dataFiles\").load_data(show_progress=True)\n",
        "\n",
        "doc_text = \"\\n\\n\".join([d.get_content() for d in documents])\n",
        "docs = [Document(text=doc_text)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS49yj1FyT08",
        "outputId": "18c58244-a45a-4c50-8583-89040c08f044"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 560.29file/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import HierarchicalNodeParser, SentenceSplitter\n",
        "\n",
        "node_parser = HierarchicalNodeParser.from_defaults()\n",
        "\n",
        "nodes = node_parser.get_nodes_from_documents(documents = docs)"
      ],
      "metadata": {
        "id": "t2S47Tia7ezO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get \"leaf\" nodes in node list. These nodes don't have children of their own."
      ],
      "metadata": {
        "id": "ak4S6iIT8OS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import get_leaf_nodes, get_root_nodes\n",
        "\n",
        "leaf_nodes = get_leaf_nodes(nodes)\n",
        "len(leaf_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xa3fwJ98Nx3",
        "outputId": "93f9d791-a2b1-40df-a5d3-8b02d703e119"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_nodes = get_root_nodes(nodes)\n",
        "len(root_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K--6Ygw68noF",
        "outputId": "37dd98db-27cb-4e5f-b526-b205058930d2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iiWz4H2x9W8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store"
      ],
      "metadata": {
        "id": "LfeepdaiNlYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Docstore: load all nodes into\n",
        "\n",
        "VectorStoreIndex: containing just the leaf-level nodes"
      ],
      "metadata": {
        "id": "ABrdeTlZNwQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storage context\n",
        "from llama_index.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.storage import StorageContext\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import VectorStoreIndex"
      ],
      "metadata": {
        "id": "ZrUvqi4uN7dv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docstore = SimpleDocumentStore()\n",
        "\n",
        "docstore.add_documents(nodes) # all nodes\n",
        "\n",
        "# define storage context (will include vector store by default too)\n",
        "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
        ")\n",
        "\n",
        "base_index = VectorStoreIndex(\n",
        "    leaf_nodes,\n",
        "    storage_context=storage_context,\n",
        "    service_context=service_context,\n",
        ")"
      ],
      "metadata": {
        "id": "iFQ1L4sdOXH-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.retrievers.auto_merging_retriever import AutoMergingRetriever\n",
        "\n",
        "base_retriever = base_index.as_retriever(similarity_top_k=6)\n",
        "retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n",
        "\n",
        "query_str = (\n",
        "    \"What did the president say about covid-19\"\n",
        ")\n",
        "\n",
        "nodes = retriever.retrieve(query_str)\n",
        "base_nodes = base_retriever.retrieve(query_str)"
      ],
      "metadata": {
        "id": "ovSvsO6ay80_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWogdSrhR-w1",
        "outputId": "60e1453f-b9f7-4cce-bc48-04c6c35c2d06"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(base_nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ONYuSk9SNS3",
        "outputId": "9b2adde7-ea2f-4c7b-94e0-5ea05713cba4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.response.notebook_utils import display_source_node\n",
        "\n",
        "for node in nodes:\n",
        "    display_source_node(node, source_length=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YIwU6sGeSR2o",
        "outputId": "48f540a8-7601-4d71-e8b1-74be2afb9b09"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** 853a9565-73a0-4125-880d-c8c3a0c109b1<br>**Similarity:** 0.8207400125552234<br>**Text:** Under these new guidelines, most Americans in most of the country can now be mask free.   \n\nAnd based on the projections, more of the country will reach that point across the next couple of weeks. \n\nThanks to the progress we have made this past year, COVID-19 need no longer control our lives.  \n\nI know some are talking about ‚Äúliving with COVID-19‚Äù. Tonight ‚Äì I say that we will never just accept living with COVID-19. \n\nWe will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard.<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** 0d6a9f53-7918-4d09-8349-78b08f46f019<br>**Similarity:** 0.8159280428328318<br>**Text:** Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n\nLast year COVID-19 kept us apart. This year we are finally together again. \n\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n\nWith a duty to one another to the American people to the Constitution. \n\nAnd with an unwavering resolve that freedom will always triumph over tyranny.<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** aa2b70e4-0a8b-4f7b-a2ec-926a20ae4939<br>**Similarity:** 0.8149870297493353<br>**Text:** Time with one another. And worst of all, so much loss of life. \n\nLet‚Äôs use this moment to reset. Let‚Äôs stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \n\nLet‚Äôs stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \n\nWe can‚Äôt change how divided we‚Äôve been. But we can change how we move forward‚Äîon COVID-19 and other issues we must face together.<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** b50609f7-1118-4239-851f-32e1a3810970<br>**Similarity:** 0.8119718233515406<br>**Text:** The pandemic has been punishing. \n\nAnd so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. \n\nI understand. \n\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. \n\nThat‚Äôs why one of the first things I did as President was fight to pass the American Rescue Plan.  \n\nBecause people were hurting. We needed to act, and we did.<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** b9e94483-7e70-47e2-89ac-56ee010fe39f<br>**Similarity:** 0.8054011110664735<br>**Text:** And, if Congress provides the funds we need, we‚Äôll have new stockpiles of tests, masks, and pills ready if needed. \n\nI cannot promise a new variant won‚Äôt come. But I can promise you we‚Äôll do everything within our power to be ready if it does.  \n\nThird ‚Äì we can end the shutdown of schools and businesses. We have the tools we need. \n\nIt‚Äôs time for Americans to get back to work and fill our great downtowns again.  People working from home can feel safe to begin to return to the office.   \n\nWe‚Äôre doing that here in the federal government.<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** 99361265-915d-4d73-9f84-9ef57b4ce6fe<br>**Similarity:** 0.8049653121329142<br>**Text:** And I know you‚Äôre tired, frustrated, and exhausted. \n\nBut I also know this. \n\nBecause of the progress we‚Äôve made, because of your resilience and the tools we have, tonight I can say  \nwe are moving forward safely, back to more normal routines.  \n\nWe‚Äôve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \n\nJust a few days ago, the Centers for Disease Control and Prevention‚Äîthe CDC‚Äîissued new mask guidelines. \n\nUnder these new guidelines, most Americans in most of the country can now be mask free.<br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "56Mcwz7Wy_kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens_eval import Feedback, Tru, TruLlama\n",
        "from trulens_eval.feedback import Groundedness\n",
        "from trulens_eval.feedback.provider.openai import OpenAI as OpenAITruLens"
      ],
      "metadata": {
        "id": "LGOdLtTYyavJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Window Retrieval"
      ],
      "metadata": {
        "id": "dRHzSKolUnjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from llama_index import (\n",
        "    SimpleDirectoryReader,\n",
        "    Document,\n",
        "    StorageContext,\n",
        "    load_index_from_storage\n",
        ")\n",
        "\n",
        "from llama_index.node_parser import SentenceWindowNodeParser\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.embeddings import OpenAIEmbedding\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import VectorStoreIndex\n",
        "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
        "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
        "\n",
        "\n",
        "# load data\n",
        "documents = SimpleDirectoryReader(input_dir=\"dataFiles\").load_data(show_progress=True)\n",
        "\n",
        "\n",
        "# merge pages into one\n",
        "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))\n",
        "\n",
        "embed_model = OpenAIEmbedding() # default embedding model ada\n",
        "llm = OpenAI(api_key=openai_api_key, model=\"gpt-3.5-turbo\")\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    embed_model=embed_model, llm=llm\n",
        ")\n",
        "\n",
        "\n",
        "def create_indexes(\n",
        "    documents: Document,\n",
        "    index_save_dir: str,\n",
        "    window_size: int = 4,\n",
        "    llm_model: str = \"gpt-3.5-turbo\",\n",
        "    temperature: float = 0.1\n",
        "):\n",
        "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
        "        window_size=window_size,\n",
        "        window_metadata_key=\"window\",\n",
        "        original_text_metadata_key=\"original_text\",\n",
        "    )\n",
        "\n",
        "\n",
        "    # creating the service context\n",
        "    sentence_context = ServiceContext.from_defaults(\n",
        "        llm=llm,\n",
        "        embed_model=embed_model,\n",
        "        node_parser=node_parser,\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(index_save_dir):\n",
        "        # creating the vector store index\n",
        "        index = VectorStoreIndex.from_documents(\n",
        "            [document], service_context=sentence_context\n",
        "        )\n",
        "\n",
        "        # make vector store persistant\n",
        "        index.storage_context.persist(persist_dir=index_save_dir)\n",
        "    else:\n",
        "        # load vector store indexed if they exist\n",
        "        index = load_index_from_storage(\n",
        "            StorageContext.from_defaults(persist_dir=index_save_dir),\n",
        "            service_context=sentence_context\n",
        "        )\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "def create_query_engine(\n",
        "    sentence_index: VectorStoreIndex,\n",
        "    similarity_top_k: int = 6,\n",
        "    rerank_top_n: int = 5,\n",
        "    rerank_model: str = \"BAAI/bge-reranker-base\",\n",
        "):\n",
        "    # add meta data replacement post processor\n",
        "    postproc = MetadataReplacementPostProcessor(\n",
        "        target_metadata_key=\"window\"\n",
        "    )\n",
        "\n",
        "    # link: https://huggingface.co/BAAI/bge-reranker-base\n",
        "    rerank = SentenceTransformerRerank(\n",
        "        top_n=rerank_top_n,\n",
        "        model=rerank_model\n",
        "    )\n",
        "\n",
        "    sentence_window_engine = sentence_index.as_query_engine(\n",
        "        similarity_top_k=similarity_top_k,\n",
        "        node_postprocessors=[postproc, rerank]\n",
        "    )\n",
        "\n",
        "    return sentence_window_engine\n",
        "\n",
        "\n",
        "# create index with window size 3\n",
        "index_window_3 = create_indexes(\n",
        "    documents=documents,\n",
        "    index_save_dir=\"storage\",\n",
        "    window_size=3,\n",
        "    llm_model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# create index with window size 6\n",
        "index_window_6 = create_indexes(\n",
        "    documents=documents,\n",
        "    index_save_dir=\"sentence_window_size_6_index\",\n",
        "    window_size=3,\n",
        "    llm_model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# create query engine\n",
        "sentence_window_engine_window_zize3 = create_query_engine(\n",
        "    sentence_index=index_window_3,\n",
        "    similarity_top_k=5,\n",
        "    rerank_top_n=2,\n",
        ")\n",
        "\n",
        "sentence_window_engine_window_zize6 = create_query_engine(\n",
        "    sentence_index=index_window_6,\n",
        "    similarity_top_k=5,\n",
        "    rerank_top_n=2,\n",
        ")\n",
        "\n",
        "response = sentence_window_engine_window_zize3.query(\n",
        "    \"What did the president say about covid-19?\"\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn8AcMdMUrJg",
        "outputId": "46ce6e70-3f58-4e22-b65e-3f7cc0473160"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 260.37file/s]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The president acknowledged that COVID-19 has impacted every decision in our lives and the life of the nation for more than two years. The president also recognized that people are tired, frustrated, and exhausted due to the ongoing pandemic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Eval window size 3\n",
        "tru_query_engine_recorder = TruLlama(sentence_window_engine_window_zize3,\n",
        "    app_id='RAG_sentence_window_size_3',\n",
        "    feedbacks=[f_groundedness, f_qa_relevance, f_context_relevance])\n",
        "\n",
        "eval_questions = []\n",
        "\n",
        "eval_questions_file = 'eval_questions.txt'\n",
        "\n",
        "with open(eval_questions_file, \"r\") as eval_qn:\n",
        "    for qn in eval_qn:\n",
        "        qn_stripped = qn.strip()\n",
        "        eval_questions.append(qn_stripped)\n",
        "\n",
        "\n",
        "def run_eval(eval_questions: List[str]):\n",
        "    for qn in eval_questions:\n",
        "        # eval using context window\n",
        "        with tru_query_engine_recorder as recording:\n",
        "            sentence_window_engine_window_zize3.query(qn)\n",
        "\n",
        "\n",
        "run_eval(eval_questions=eval_questions)\n",
        "\n",
        "# run dashboard\n",
        "tru.run_dashboard()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwaZu2hbX28m",
        "outputId": "66d228ab-59d5-4b28-8da2-d0e233e4f966"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:trulens_eval.tru:Tru was already initialized. Cannot change database_url=None or database_file=benchmark.sqlite .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ In groundedness_measure_with_cot_reasons, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
            "‚úÖ In groundedness_measure_with_cot_reasons, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "‚úÖ In relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "‚úÖ In relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "‚úÖ In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
            "‚úÖ In qs_relevance, input statement will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Eval window size 6\n",
        "tru_query_engine_recorder = TruLlama(sentence_window_engine_window_zize6,\n",
        "    app_id='RAG_sentence_window_size_6',\n",
        "    feedbacks=[f_groundedness, f_qa_relevance, f_context_relevance])\n",
        "\n",
        "eval_questions = []\n",
        "\n",
        "eval_questions_file = 'eval_questions.txt'\n",
        "\n",
        "with open(eval_questions_file, \"r\") as eval_qn:\n",
        "    for qn in eval_qn:\n",
        "        qn_stripped = qn.strip()\n",
        "        eval_questions.append(qn_stripped)\n",
        "\n",
        "\n",
        "def run_eval(eval_questions: List[str]):\n",
        "    for qn in eval_questions:\n",
        "        # eval using context window\n",
        "        with tru_query_engine_recorder as recording:\n",
        "            sentence_window_engine_window_zize6.query(qn)\n",
        "\n",
        "\n",
        "run_eval(eval_questions=eval_questions)\n",
        "\n",
        "# run dashboard\n",
        "tru.run_dashboard()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eVwyb15pLWn",
        "outputId": "03926b29-6381-4028-d578-777891c8dd09"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dashboard ...\n",
            "Config file already exists. Skipping writing process.\n",
            "Credentials file already exists. Skipping writing process.\n",
            "Dashboard already running at path:   Submit this IP Address: 34.73.66.145\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}